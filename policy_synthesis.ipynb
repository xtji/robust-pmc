{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Policy Synthesis in a Finite MDP  \n",
    "This notebook compares these two methods, **Risk-neutral Value Iteration vs Robust Risk-sensitive Synthesis (Cantelli, 2 moments)**, on the same finite MDP (a betting-game example adapted from PRISM case study):\n",
    "\n",
    "1. **Risk-neutral Value Iteration (baseline)**: Optimizes the expected cumulative reward/cost.\n",
    "\n",
    "2. **Robust Risk-sensitive Value Iteration (our method)**: Optimizes expected performance while enforcing a conservative lower bound on a *chance constraint* using the **(one-sided) Cantelli inequality**."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b8df532b004f828"
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from typing import Optional, Dict, Tuple\n",
    "from mdp import MDP, build_betting_game_mdp\n",
    "\n",
    "GLOBAL_SEED = 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T20:10:54.470805Z",
     "start_time": "2026-01-11T20:10:54.438521Z"
    }
   },
   "id": "b4eea9168c9560ed"
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "def value_iteration(\n",
    "    mdp: MDP,\n",
    "    tol: float = 1e-9,\n",
    "    max_iter: int = 500,\n",
    "    objective: str = \"min\",  # \"min\" (cost) or \"max\" (reward)\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Standard Value Iteration on a dense MDP.\n",
    "\n",
    "    objective=\"min\": V(s) = min_a E[R + gamma V(s')]\n",
    "    objective=\"max\": V(s) = max_a E[R + gamma V(s')]\n",
    "\n",
    "    Returns:\n",
    "        pi: greedy deterministic policy (S,)\n",
    "        V : value function (S,)\n",
    "    \"\"\"\n",
    "    objective = objective.lower()\n",
    "    if objective not in (\"min\", \"max\"):\n",
    "        raise ValueError(\"objective must be 'min' or 'max'\")\n",
    "\n",
    "    S, A = mdp.n_states, mdp.n_actions\n",
    "    V = np.zeros(S, dtype=float)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        V_old = V.copy()\n",
    "        Q = np.zeros((S, A), dtype=float)\n",
    "\n",
    "        for a in range(A):\n",
    "            Q[:, a] = (mdp.P[:, a, :] * (mdp.R[:, a, :] + mdp.gamma * V_old[None, :])).sum(axis=1)\n",
    "\n",
    "        if objective == \"min\":\n",
    "            V = Q.min(axis=1)\n",
    "        else:\n",
    "            V = Q.max(axis=1)\n",
    "\n",
    "        if np.max(np.abs(V - V_old)) <= tol:\n",
    "            break\n",
    "\n",
    "    # greedy policy extraction\n",
    "    pi = np.zeros(S, dtype=int)\n",
    "    for s in range(S):\n",
    "        if mdp.terminal[s]:\n",
    "            pi[s] = 0\n",
    "            continue\n",
    "\n",
    "        q_sa = np.zeros(A, dtype=float)\n",
    "        for a in range(A):\n",
    "            q_sa[a] = float(mdp.P[s, a, :] @ (mdp.R[s, a, :] + mdp.gamma * V))\n",
    "\n",
    "        pi[s] = int(np.argmin(q_sa)) if objective == \"min\" else int(np.argmax(q_sa))\n",
    "\n",
    "    return pi, V"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T20:10:54.484467Z",
     "start_time": "2026-01-11T20:10:54.452945Z"
    }
   },
   "id": "4bd47eb10974cb53"
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [],
   "source": [
    "def evaluate_policy_mean_var(mdp: MDP, pi: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Exact (linear-system) evaluation of mean and second raw moment under a fixed policy.\n",
    "\n",
    "    Returns:\n",
    "        v  : E[G | s]\n",
    "        m2 : E[G^2 | s]\n",
    "    \"\"\"\n",
    "    S = mdp.n_states\n",
    "    T = np.where(~mdp.terminal)[0]  # transient\n",
    "    Z = np.where(mdp.terminal)[0]   # terminal\n",
    "\n",
    "    Ppi = np.zeros((S, S), dtype=float)\n",
    "    b = np.zeros(S, dtype=float)\n",
    "    r2 = np.zeros(S, dtype=float)\n",
    "\n",
    "    for s in range(S):\n",
    "        a = int(pi[s])\n",
    "        Prow = mdp.P[s, a]\n",
    "        Rrow = mdp.R[s, a]\n",
    "        Ppi[s, :] = Prow\n",
    "        b[s] = float(Prow @ Rrow)               # E[r]\n",
    "        r2[s] = float(Prow @ (Rrow * Rrow))     # E[r^2]\n",
    "\n",
    "    # Mean: v_T = (I - gamma B)^(-1) b_T\n",
    "    v = np.zeros(S, dtype=float)\n",
    "    if T.size:\n",
    "        B = Ppi[np.ix_(T, T)]\n",
    "        A = np.eye(T.size) - mdp.gamma * B\n",
    "        v[T] = np.linalg.solve(A, b[T])\n",
    "    v[Z] = 0.0\n",
    "\n",
    "    # Cross term: E[r * v(s')]\n",
    "    ErV = np.zeros(S, dtype=float)\n",
    "    for s in T:\n",
    "        a = int(pi[s])\n",
    "        ErV[s] = float(mdp.P[s, a] @ (mdp.R[s, a] * v))\n",
    "\n",
    "    # Second moment: m2_T = (I - gamma^2 B)^(-1) [ E[r^2] + 2 gamma E[r v(s')] ]\n",
    "    m2 = np.zeros(S, dtype=float)\n",
    "    if T.size:\n",
    "        B = Ppi[np.ix_(T, T)]\n",
    "        A2 = np.eye(T.size) - (mdp.gamma ** 2) * B\n",
    "        rhs = r2[T] + 2.0 * mdp.gamma * ErV[T]\n",
    "        m2[T] = np.linalg.solve(A2, rhs)\n",
    "    m2[Z] = 0.0\n",
    "\n",
    "    return v, m2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T20:10:54.484955Z",
     "start_time": "2026-01-11T20:10:54.459057Z"
    }
   },
   "id": "183e2deb17639df6"
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [
    "def sample_returns(\n",
    "    mdp: MDP,\n",
    "    pi: np.ndarray,\n",
    "    n_episodes: int = 10_000,\n",
    "    max_steps: int = 1_000,\n",
    "    rng: Optional[np.random.Generator] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Sample discounted returns G under deterministic policy pi for CVaR estimation\"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    S = mdp.n_states\n",
    "    returns = np.zeros(n_episodes, dtype=float)\n",
    "\n",
    "    for ep in range(n_episodes):\n",
    "        s = mdp.start_state\n",
    "        G = 0.0\n",
    "        disc = 1.0\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            if mdp.terminal[s]:\n",
    "                break\n",
    "            a = int(pi[s])\n",
    "            probs = mdp.P[s, a, :]\n",
    "            s_next = rng.choice(S, p=probs)\n",
    "            r = float(mdp.R[s, a, s_next])\n",
    "\n",
    "            G += disc * r\n",
    "            disc *= mdp.gamma\n",
    "            s = s_next\n",
    "\n",
    "        returns[ep] = G\n",
    "\n",
    "    return returns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T20:10:54.485968Z",
     "start_time": "2026-01-11T20:10:54.468756Z"
    }
   },
   "id": "b2620b0e651457e1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chance constraint (conservative bound)\n",
    "We target a constraint of the form **R\\_min:** $\\Pr(G^\\pi \\le \\tau) \\ge \\alpha$ or **R\\_max:** $\\Pr(G^\\pi \\ge \\tau) \\ge \\alpha$. During synthesis we use the Cantelli’s inequality with $n=2$ to obtain a conservative lower bound \\(\\rho(\\pi)\\) on the satisfaction probability."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f2bb89949d6dd4b"
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "def constraint_satisfied(\n",
    "    mu0: float,\n",
    "    std0: float,\n",
    "    tau: float,\n",
    "    alpha: float,\n",
    "    alpha_cvar: float,\n",
    "    returns: Optional[np.ndarray],\n",
    "    objective: str = \"min\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Report:\n",
    "      - Cantelli lower bound (prob_lb)\n",
    "      - empirical chance probability (prob_emp)\n",
    "      - empirical CVaR_alpha_cvar (cvar_alpha)\n",
    "    \"\"\"\n",
    "    objective = objective.lower()\n",
    "    if objective not in (\"min\", \"max\"):\n",
    "        raise ValueError(\"objective must be 'min' or 'max'\")\n",
    "\n",
    "    var0 = float(std0 ** 2)\n",
    "\n",
    "    def cantelli_right_lb(mu: float, std: float, tau_: float) -> float:\n",
    "        gap = mu - tau_\n",
    "        if std <= 1e-15:\n",
    "            return float(gap >= 0.0)\n",
    "        if gap <= 0.0:\n",
    "            return 0.0\n",
    "        return float((gap * gap) / (std * std + gap * gap))\n",
    "\n",
    "    # Cantelli bound on the chance constraint\n",
    "    if objective == \"max\":\n",
    "        prob_lb = cantelli_right_lb(mu0, std0, tau)          # P(G >= tau)\n",
    "    else:\n",
    "        prob_lb = cantelli_right_lb(-mu0, std0, -tau)        # P(G <= tau) == P(-G >= -tau)\n",
    "\n",
    "    satisfied_cantelli = bool(prob_lb >= alpha)\n",
    "\n",
    "    def empirical_cvar(samples: np.ndarray, alpha_level: float, objective: str) -> float:\n",
    "        xs = np.sort(np.asarray(samples, dtype=float))\n",
    "        N = xs.size\n",
    "        tail_mass = max(1e-6, 1.0 - alpha_level)\n",
    "        k = max(1, int(np.ceil(tail_mass * N)))\n",
    "        if objective == \"max\":\n",
    "            tail = xs[:k]     # worst = smallest\n",
    "        else:\n",
    "            tail = xs[-k:]    # worst = largest\n",
    "        return float(tail.mean())\n",
    "\n",
    "    prob_emp = None\n",
    "    satisfied_emp = None\n",
    "\n",
    "    if returns is None or len(returns) == 0:\n",
    "        cvar_alpha = float(mu0)\n",
    "    else:\n",
    "        xs = np.asarray(returns, dtype=float)\n",
    "        if objective == \"max\":\n",
    "            prob_emp = float(np.mean(xs >= tau))\n",
    "        else:\n",
    "            prob_emp = float(np.mean(xs <= tau))\n",
    "\n",
    "        satisfied_emp = bool(prob_emp >= alpha)\n",
    "        cvar_alpha = empirical_cvar(xs, alpha_cvar, objective)\n",
    "\n",
    "    return satisfied_cantelli, float(prob_lb), var0, float(cvar_alpha), prob_emp, satisfied_emp"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T20:10:54.486088Z",
     "start_time": "2026-01-11T20:10:54.479361Z"
    }
   },
   "id": "74f3cdf23200ea03"
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "def robust_vi(\n",
    "    mdp,\n",
    "    alpha: float,\n",
    "    tau: float,\n",
    "    beta: float = 10.0,          # c-update multiplier/divider (paper Eq. c-update)\n",
    "    eps: float = 1e-6,           # barrier slack\n",
    "    max_outer: int = 50,\n",
    "    R_max: int= 100.0,\n",
    "    vi_tol: float = 1e-9,\n",
    "    verbose: bool = False,\n",
    "    tol_prob: float = 1e-6,\n",
    "    lambda_init: float = 1.0,    # initial c\n",
    "    tie_noise: float = 1e-9,\n",
    "    hysteresis_eps: float = 1e-12,\n",
    "    max_eval_iters: int = 200,\n",
    "    c_max: float = 5e4,\n",
    "    c_min: float = 1e-6,\n",
    "    change_frac_tol: float = 0.0,\n",
    "    min_outer_iters: int = 5,\n",
    "    rho_stall_patience: int = 5,\n",
    "    objective: str = \"min\",\n",
    "    pi_init: Optional[np.ndarray] = None,\n",
    ") -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    Cantelli-based barrier-shaped policy improvement (Algorithm-1).\n",
    "\n",
    "    Returns keys compatible with your old printing:\n",
    "        pi, mu0, std0, J, prob_lb, outer_iters, lambda\n",
    "    where:\n",
    "        J = best barrier objective L_c (smaller is better in both modes after sign handling)\n",
    "    \"\"\"\n",
    "    objective = objective.lower()\n",
    "    if objective not in (\"min\", \"max\"):\n",
    "        raise ValueError(\"objective must be 'min' or 'max'\")\n",
    "\n",
    "    S, A = int(mdp.n_states), int(mdp.n_actions)\n",
    "    s0 = int(mdp.start_state)\n",
    "    P = np.asarray(mdp.P, dtype=float)\n",
    "    R = np.asarray(mdp.R, dtype=float)\n",
    "    gamma = float(mdp.gamma)\n",
    "    terminal = np.asarray(mdp.terminal, dtype=bool)\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    # ------------------ Cantelli probability lower bound ------------------\n",
    "    def cantelli_right_lb(mu: float, std: float, tau_: float) -> float:\n",
    "        gap = mu - tau_\n",
    "        if std <= 1e-15:\n",
    "            return 1.0 if gap >= 0.0 else 0.0\n",
    "        if gap <= 0.0:\n",
    "            return 0.0\n",
    "        den = std * std + gap * gap\n",
    "        return (gap * gap) / den if den > 0 else 1.0\n",
    "\n",
    "    def chance_prob_lb(mu0: float, std0: float) -> float:\n",
    "        if objective == \"max\":\n",
    "            return cantelli_right_lb(mu0, std0, tau)\n",
    "        else:\n",
    "            return cantelli_right_lb(-mu0, std0, -tau)\n",
    "\n",
    "    # ------------------ Paper-like barrier objective L_c ------------------\n",
    "    def barrier_Lc(mu0: float, std0: float, c: float) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Returns (Lc, rho). Lc is a scalar cost to minimize.\n",
    "        We use a piecewise penalty so infeasible region has non-vanishing push.\n",
    "        \"\"\"\n",
    "        rho = chance_prob_lb(mu0, std0)\n",
    "        x = rho - alpha\n",
    "\n",
    "        # base term: objective handling\n",
    "        if objective == \"min\":\n",
    "            base = mu0\n",
    "        else:\n",
    "            base = -mu0  # maximize mu0 <=> minimize -mu0\n",
    "\n",
    "        if x >= 0.0:\n",
    "            # log barrier inside feasible region\n",
    "            Lc = base - c * np.log(x + eps)\n",
    "        else:\n",
    "            # linear extension outside feasible region\n",
    "            Lc = base - ((c * x) / eps + c * np.log(eps) - R_max)\n",
    "\n",
    "        return float(Lc), float(rho)\n",
    "\n",
    "    # ------------------ Policy evaluation: u1,u2 ------------------\n",
    "    def policy_eval_u1_u2(pi: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        u1 = np.zeros(S, dtype=float)\n",
    "        u2 = np.zeros(S, dtype=float)\n",
    "\n",
    "        Ppi = np.zeros((S, S), dtype=float)\n",
    "        Rpi = np.zeros((S, S), dtype=float)\n",
    "        for s in range(S):\n",
    "            a = int(pi[s])\n",
    "            Ppi[s, :] = P[s, a, :]\n",
    "            Rpi[s, :] = R[s, a, :]\n",
    "\n",
    "        ER = (Ppi * Rpi).sum(axis=1)\n",
    "        ER2 = (Ppi * (Rpi ** 2)).sum(axis=1)\n",
    "\n",
    "        for _ in range(max_eval_iters):\n",
    "            u1_old = u1.copy()\n",
    "            u2_old = u2.copy()\n",
    "\n",
    "            u1 = ER + gamma * (Ppi @ u1_old)\n",
    "            ERu1 = (Ppi * Rpi) @ u1_old\n",
    "            u2 = ER2 + 2.0 * gamma * ERu1 + (gamma ** 2) * (Ppi @ u2_old)\n",
    "\n",
    "            u1[terminal] = 0.0\n",
    "            u2[terminal] = 0.0\n",
    "\n",
    "            if max(np.max(np.abs(u1 - u1_old)), np.max(np.abs(u2 - u2_old))) <= vi_tol:\n",
    "                break\n",
    "\n",
    "        return u1, u2\n",
    "\n",
    "    # ------------------ One-step propagated moments Q1,Q2 ------------------\n",
    "    def one_step_Q1_Q2(u1: np.ndarray, u2: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        Q1 = np.zeros((S, A), dtype=float)\n",
    "        Q2 = np.zeros((S, A), dtype=float)\n",
    "\n",
    "        for a in range(A):\n",
    "            Psa = P[:, a, :]\n",
    "            Rsa = R[:, a, :]\n",
    "\n",
    "            ER = (Psa * Rsa).sum(axis=1)\n",
    "            ER2 = (Psa * (Rsa ** 2)).sum(axis=1)\n",
    "\n",
    "            EV1 = Psa @ u1\n",
    "            ERu1 = (Psa * Rsa) @ u1\n",
    "            EV2 = Psa @ u2\n",
    "\n",
    "            Q1[:, a] = ER + gamma * EV1\n",
    "            Q2[:, a] = ER2 + 2.0 * gamma * ERu1 + (gamma ** 2) * EV2\n",
    "\n",
    "        Q1[terminal, :] = 0.0\n",
    "        Q2[terminal, :] = 0.0\n",
    "        return Q1, Q2\n",
    "\n",
    "    # ------------------ Compute k1,k2 from dL/du (n=2) ------------------\n",
    "    def compute_k1_k2(u1_s0: float, u2_s0: float, c: float) -> Tuple[float, float]:\n",
    "        mu0 = float(u1_s0)\n",
    "        m2_0 = float(u2_s0)\n",
    "        var0 = max(m2_0 - mu0 * mu0, 0.0)\n",
    "        std0 = float(np.sqrt(var0))\n",
    "\n",
    "        # unify direction for rho (right-tail Cantelli)\n",
    "        if objective == \"max\":\n",
    "            mu_tilde, tau_tilde, sign_mu = mu0, tau, +1.0\n",
    "        else:\n",
    "            mu_tilde, tau_tilde, sign_mu = -mu0, -tau, -1.0\n",
    "\n",
    "        Delta = mu_tilde - tau_tilde\n",
    "        if Delta <= 0.0 or std0 <= 1e-15:\n",
    "            rho = 0.0\n",
    "            drho_du1 = 0.0\n",
    "            drho_du2 = 0.0\n",
    "        else:\n",
    "            D = max(var0 + Delta * Delta, 1e-12)\n",
    "            rho = (Delta * Delta) / D\n",
    "\n",
    "            drho_dmu_tilde = (2.0 * Delta * var0 + 2.0 * mu_tilde * (Delta * Delta)) / (D * D)\n",
    "            drho_du1 = sign_mu * drho_dmu_tilde\n",
    "            drho_du2 = -(Delta * Delta) / (D * D)\n",
    "\n",
    "        # derivative of barrier w.r.t rho:\n",
    "        # feasible: -c/(rho-alpha+eps)\n",
    "        # infeasible: -(c/eps)  (linear extension)\n",
    "        if rho - alpha >= 0.0:\n",
    "            dLc_drho = -c / (rho - alpha + eps)\n",
    "        else:\n",
    "            dLc_drho = -c / eps\n",
    "\n",
    "        # base term derivative: d(base)/du1 is +1 (min) or -1 (max), and d(base)/du2 is 0\n",
    "        if objective == \"min\":\n",
    "            dbase_du1 = 1.0\n",
    "        else:\n",
    "            dbase_du1 = -1.0\n",
    "\n",
    "        k1 = dbase_du1 + dLc_drho * drho_du1\n",
    "        k2 = dLc_drho * drho_du2\n",
    "        return float(k1), float(k2)\n",
    "\n",
    "    # ------------------ Greedy policy improvement ------------------\n",
    "    def greedy_improve(pi: np.ndarray, u1: np.ndarray, u2: np.ndarray, c: float) -> Tuple[np.ndarray, float]:\n",
    "        k1, k2 = compute_k1_k2(u1[s0], u2[s0], c)\n",
    "        Q1, Q2 = one_step_Q1_Q2(u1, u2)\n",
    "\n",
    "        J = k1 * Q1 + k2 * Q2  # cost-to-minimize surrogate\n",
    "\n",
    "        if tie_noise > 0:\n",
    "            J = J + tie_noise * rng.standard_normal(J.shape)\n",
    "\n",
    "        pi_new = pi.copy()\n",
    "        switches = 0\n",
    "        for s in range(S):\n",
    "            if terminal[s]:\n",
    "                pi_new[s] = 0\n",
    "                continue\n",
    "\n",
    "            a_cur = int(pi[s])\n",
    "            a_best = int(np.argmin(J[s, :]))\n",
    "\n",
    "            # hysteresis: switch only if meaningfully better\n",
    "            if J[s, a_best] + hysteresis_eps < J[s, a_cur]:\n",
    "                pi_new[s] = a_best\n",
    "                switches += 1\n",
    "\n",
    "        changed_frac = switches / max(1, np.sum(~terminal))\n",
    "        return pi_new, float(changed_frac)\n",
    "\n",
    "    # ------------------ Initialization (same as before) ------------------\n",
    "    if pi_init is None:\n",
    "        pi = np.zeros(S, dtype=int)\n",
    "    else:\n",
    "        pi = np.array(pi_init, dtype=int).copy()\n",
    "    pi[terminal] = 0\n",
    "\n",
    "    c = float(max(lambda_init, c_min))\n",
    "\n",
    "    # Evaluate initial policy\n",
    "    u1, u2 = policy_eval_u1_u2(pi)\n",
    "    mu0 = float(u1[s0])\n",
    "    std0 = float(np.sqrt(max(float(u2[s0] - mu0 * mu0), 0.0)))\n",
    "    Lc, rho = barrier_Lc(mu0, std0, c)\n",
    "\n",
    "    best_pi = pi.copy()\n",
    "    best_mu0, best_std0 = mu0, std0\n",
    "    best_rho = rho\n",
    "    best_J = Lc\n",
    "\n",
    "    # stopping helpers\n",
    "    prev_rho = rho\n",
    "    rho_stall = 0\n",
    "\n",
    "    # ------------------ Outer iterations ------------------\n",
    "    for it in range(1, max_outer + 1):\n",
    "        # Greedy improvement under current c\n",
    "        pi_new, changed_frac = greedy_improve(pi, u1, u2, c)\n",
    "\n",
    "        # Evaluate new policy\n",
    "        u1_new, u2_new = policy_eval_u1_u2(pi_new)\n",
    "        mu0_new = float(u1_new[s0])\n",
    "        std0_new = float(np.sqrt(max(float(u2_new[s0] - mu0_new * mu0_new), 0.0)))\n",
    "        Lc_new, rho_new = barrier_Lc(mu0_new, std0_new, c)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[OUT] it={it:02d} c={c:.6g} mu0={mu0_new:.6f} rho={rho_new:.6f} \"\n",
    "                  f\"Lc={Lc_new:.6f} {'FEAS' if rho_new >= alpha else 'INF'} \"\n",
    "                  f\"chg={changed_frac*100:.2f}%\")\n",
    "\n",
    "        # Best-so-far by barrier objective\n",
    "        if Lc_new < best_J - 1e-12:\n",
    "            best_pi = pi_new.copy()\n",
    "            best_mu0, best_std0 = mu0_new, std0_new\n",
    "            best_rho = rho_new\n",
    "            best_J = Lc_new\n",
    "\n",
    "        # Update c (paper-style)\n",
    "        if rho_new < alpha:\n",
    "            c = min(beta * c, c_max)\n",
    "        else:\n",
    "            c = max(c / beta, c_min)\n",
    "\n",
    "        # Convergence tracking\n",
    "        if abs(rho_new - prev_rho) <= tol_prob:\n",
    "            rho_stall += 1\n",
    "        else:\n",
    "            rho_stall = 0\n",
    "        prev_rho = rho_new\n",
    "\n",
    "        # Advance iterate\n",
    "        pi_unchanged = np.array_equal(pi_new, pi)\n",
    "        pi = pi_new\n",
    "        u1, u2 = u1_new, u2_new\n",
    "\n",
    "        # Stopping (do not stop too early)\n",
    "        if it >= min_outer_iters:\n",
    "            if pi_unchanged and changed_frac <= change_frac_tol:\n",
    "                if verbose:\n",
    "                    print(\"[STOP] policy unchanged.\")\n",
    "                break\n",
    "            if rho_stall >= rho_stall_patience:\n",
    "                if verbose:\n",
    "                    print(\"[STOP] rho stagnated.\")\n",
    "                break\n",
    "\n",
    "    return {\n",
    "        \"pi\": best_pi,\n",
    "        \"mu0\": float(best_mu0),\n",
    "        \"std0\": float(best_std0),\n",
    "        \"prob_lb\": float(best_rho),\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T20:10:54.616228Z",
     "start_time": "2026-01-11T20:10:54.614872Z"
    }
   },
   "id": "6f07b542e3e09037"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running the comparison\n",
    "We build the betting-game MDP, run the risk-neutral value iteration, then the robust risk-sensitive VI with a Cantelli-based chance constraint. For each method, we report the expected reward, the variance / std, a conservative satisfaction lower bound (Cantelli), empirical CVaR and runtime."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "705bc6462fbecbc7"
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP: S=1011, A=7, gamma=1.0, start=5, terminals=1\n",
      "\n",
      "[RISK-NEUTRAL VALUE ITERATION]\n",
      "time=0.314973s\n",
      "mu0=61.921383, std0=29.778478, var=886.757735, prob_lb(Cantelli)=0.620515,CVaRα(emp)=92.735510, satisfies(Cantelli)=False\n",
      "\n",
      "[Robust RISK-SENSITIVE VALUE ITERATION]\n",
      "time=5.790376s\n",
      "mu0=71.391637, std0=17.759828, var=315.411500, prob_lb(Cantelli)=0.721823,CVaRα(emp)=90.596935, satisfies(Cantelli)=True\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Build environment\n",
    "# -----------------------------\n",
    "mdp = build_betting_game_mdp(\n",
    "    M=5, MAX_MONEY=100, STAGES=10,\n",
    "    p_win=0.7, p_jackpot=0.05, gamma=1.0\n",
    ")\n",
    "\n",
    "s0 = mdp.start_state\n",
    "print(f\"MDP: S={mdp.n_states}, A={mdp.n_actions}, gamma={mdp.gamma}, start={s0}, terminals={int(np.sum(mdp.terminal))}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Settings\n",
    "# -----------------------------\n",
    "alpha = 0.7\n",
    "tau = 100.0\n",
    "alpha_cvar = 0.7\n",
    "N_samples = 5000\n",
    "beta = 10\n",
    "eps = 1e-6\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Risk-neutral Value Iteration\n",
    "# -----------------------------\n",
    "t0 = time.perf_counter()\n",
    "pi_rn, V_rn = value_iteration(mdp, tol=1e-2, max_iter=50, objective=\"min\")\n",
    "v_mu_rn, v_m2_rn = evaluate_policy_mean_var(mdp, pi_rn)\n",
    "mu0_rn = float(v_mu_rn[s0])\n",
    "std0_rn = float(np.sqrt(max(float(v_m2_rn[s0] - mu0_rn * mu0_rn), 0.0)))\n",
    "rn_time = time.perf_counter() - t0\n",
    "\n",
    "returns_rn = sample_returns(\n",
    "    mdp, pi_rn, n_episodes=N_samples, max_steps=1000,\n",
    "    rng=np.random.default_rng(GLOBAL_SEED)\n",
    ")\n",
    "rn_ok_cant, rn_prob_lb, rn_var, rn_cvar, rn_prob_emp, rn_ok_emp = constraint_satisfied(\n",
    "    mu0_rn, std0_rn, tau, alpha, alpha_cvar, returns_rn, objective=\"min\"\n",
    ")\n",
    "\n",
    "print(\"\\n[RISK-NEUTRAL VALUE ITERATION]\")\n",
    "print(f\"time={rn_time:.6f}s\")\n",
    "print(\n",
    "    f\"mu0={mu0_rn:.6f}, std0={std0_rn:.6f}, var={rn_var:.6f}, \"\n",
    "    f\"prob_lb(Cantelli)={rn_prob_lb:.6f},\"\n",
    "    f\"CVaRα(emp)={rn_cvar:.6f}, \"\n",
    "    f\"satisfies(Cantelli)={rn_ok_cant}\"\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) ROBUST RISK-SENSITIVE-VI (Cantelli chance constraint)\n",
    "# -----------------------------\n",
    "t1 = time.perf_counter()\n",
    "res_rsvi = robust_vi(\n",
    "    mdp,\n",
    "    alpha=alpha,\n",
    "    tau=tau,\n",
    "    beta=beta,\n",
    "    eps=eps,\n",
    "    max_outer=50,\n",
    "    vi_tol=1e-2,\n",
    "    objective=\"min\",\n",
    ")\n",
    "rs_time = time.perf_counter() - t1\n",
    "\n",
    "returns_rs = sample_returns(\n",
    "    mdp, res_rsvi[\"pi\"], n_episodes=N_samples, max_steps=1000,\n",
    "    rng=np.random.default_rng(GLOBAL_SEED)\n",
    ")\n",
    "rs_ok_cant, rs_prob_lb, rs_var, rs_cvar, rs_prob_emp, rs_ok_emp = constraint_satisfied(\n",
    "    res_rsvi[\"mu0\"], res_rsvi[\"std0\"], tau, alpha, alpha_cvar, returns_rs, objective=\"min\"\n",
    ")\n",
    "\n",
    "print(\"\\n[Robust RISK-SENSITIVE VALUE ITERATION]\")\n",
    "print(f\"time={rs_time:.6f}s\")\n",
    "print(\n",
    "    f\"mu0={res_rsvi['mu0']:.6f}, std0={res_rsvi['std0']:.6f}, var={rs_var:.6f}, \"\n",
    "    f\"prob_lb(Cantelli)={rs_prob_lb:.6f},\"\n",
    "    f\"CVaRα(emp)={rs_cvar:.6f}, \"\n",
    "    f\"satisfies(Cantelli)={rs_ok_cant}\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T20:11:33.249489Z",
     "start_time": "2026-01-11T20:11:25.394734Z"
    }
   },
   "id": "2070386277a8c2be"
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T20:11:02.883652Z",
     "start_time": "2026-01-11T20:11:02.882306Z"
    }
   },
   "id": "fbf4b96bd9fcee02"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
